# Semantic Drone Dataset Segmentation using U-Net

This repository contains an implementation of the U-Net architecture for semantic segmentation of drone images from the Semantic Drone Dataset. The project aims to segment various classes such as buildings, roads, trees, and vehicles from aerial images.

The goal of semantic image segmentation is to label each pixel of an image with a corresponding class of what is being represented. Because weâ€™re predicting for every pixel in the image, this task is commonly referred to as dense prediction.

The expected output in semantic segmentation are not just labels and bounding box parameters. The output itself is a high resolution image (typically of the same size as input image) in which each pixel is classified to a particular class. Thus it is a pixel level image classification.

## Dataset

The dataset can be obtained from the following link:

[Semantic Drone Dataset](https://www.kaggle.com/datasets/bulentsiyah/semantic-drone-dataset)

The Semantic Drone Dataset focuses on enhancing the safety of autonomous drone navigation and landing procedures by emphasizing semantic comprehension of urban environments. The dataset comprises imagery showcasing over 20 houses captured from a bird's eye view at altitudes ranging from 5 to 30 meters above ground level. Images are obtained using a high-resolution camera, yielding a size of 6000x4000 pixels (24 megapixels). The training dataset encompasses 400 publicly accessible images, while the test dataset comprises 200 private images.

For person detection, the dataset includes bounding box annotations for both the training and test sets. Additionally, pixel-accurate annotations are provided for semantic segmentation tasks for the same sets. The dataset's complexity is streamlined to 20 distinct classes, as outlined in Table 1 below.

### Table 1: Semantic Classes of the Drone Dataset

1. Tree
2. Grass
3. Other vegetation
4. Dirt
5. Gravel
6. Rocks
7. Water
8. Paved area
9. Pool
10. Person
11. Dog
12. Car
13. Bicycle
14. Roof
15. Wall
16. Fence
17. Fence pole
18. Window
19. Door
20. Obstacle

## Requirements

- Python 3.x
- TensorFlow
- Keras
- OpenCV
- Albumentations
- Scikit-learn
- Pandas
- Numpy
- Matplotlib

## Usage

1. Clone the repository:

bash
git clone https://github.com/Keshiiika/sementic_segmentation_drone_images.git
cd semantic-segmentation-drone-images


2. Download the Semantic Drone Dataset and place it in the appropriate directory.

3. Run the Jupyter Notebook semantic_segmentation.ipynb to preprocess the data, train the U-Net model, and generate segmentation masks.

## Model Architecture

The U-Net architecture consists of an encoder and a decoder, with skip connections between the encoder and decoder layers. The encoder downsamples the input image, while the decoder upsamples the feature maps to produce the segmentation masks.

The model architecture can be customized by modifying the number of filters and layers in the build_unet function.

## Data Preprocessing

The dataset is preprocessed using the augment_data function, which performs data augmentation techniques such as random cropping, horizontal flipping, and vertical flipping. The preprocessed images and masks are stored in the ./new_data directory.

## Training

The model is trained using the tf_dataset function, which creates a TensorFlow dataset pipeline for efficient data loading and preprocessing. The training process can be monitored using TensorFlow callbacks such as ModelCheckpoint, ReduceLROnPlateau, and EarlyStopping.

## Results

The trained model generates segmentation masks for the test images, which are saved in the ./results directory. The segmentation masks can be visualized and evaluated using appropriate metrics.

Here are some example results:

<p align="center">
  <img src="https://github.com/Keshiiika/sementic_segmentation_drone_images/blob/main/results.png" alt="Original Image">
  <img src="https://github.com/Keshiiika/sementic_segmentation_drone_images/blob/main/result%202.png" alt="Original Image">
  <img src="https://github.com/Keshiiika/sementic_segmentation_drone_images/blob/main/result%203.png" alt="Original Image">
  <img src="https://github.com/Keshiiika/sementic_segmentation_drone_images/blob/main/result%204.png" alt="Original Image">
  <img src="https://github.com/Keshiiika/sementic_segmentation_drone_images/blob/main/result%205.png" alt="Original Image">
  <img src="https://github.com/Keshiiika/sementic_segmentation_drone_images/blob/main/result%206.png" alt="Original Image">
  <img src="https://github.com/Keshiiika/sementic_segmentation_drone_images/blob/main/result%207.png" alt="Original Image">
</p>

The left image shows the original input image, the middle image shows the ground truth segmentation mask, and the right image shows the predicted segmentation mask generated by the U-Net model.

## Contributions

Contributions to this project are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.

## License

This project is licensed under the [MIT License](LICENSE).
